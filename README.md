# ðŸ§  Neural Network from Scratch

A flexible, modular implementation of a fully-connected neural network built **from scratch using NumPy**.

---

## ðŸš€ Features

- âœ… **Multi-layer Neural Network**: Define architectures with any number of layers and units.
- ðŸ§© **Multiple Activation Functions**: Supports `Sigmoid`, `ReLU`, and `Tanh`.
- âš™ï¸ **Weight Initialization**: Uses Xavier for sigmoid/tanh and He for ReLU.
- â›“ **Mini-batch Gradient Descent**: Configurable batch sizes for efficient training.
- ðŸ“Š **Binary Cross-Entropy Loss**: Ideal for binary classification tasks.
- ðŸ§  **Mathematically Correct Backpropagation**: Gradient computation is cleanly separated from parameter updates.

---

## ðŸ“‚ Project Structure

```plaintext
neural-network-scratch/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ net_layer.py          # Layer class: forward/backward propagation
â”‚   â””â”€â”€ neural_network.py     # NeuralNetwork class (configurable & correct)
â”œâ”€â”€ optimizers/
â”‚   â””â”€â”€ mini_batch_sgd.py     # Mini-batch SGD optimizer
â”œâ”€â”€ losses/
â”‚   â””â”€â”€ cross_entropy.py      # Binary cross-entropy loss
â”œâ”€â”€ example_usage.py          # Example training & evaluation script
â”œâ”€â”€ main.py                   # Entry point to run your custom setup
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                 # You're here
```

---

## ðŸ”§ Components

### 1. `Layer` â€“ `models/net_layer.py`
The core building block of the network:
- Proper weight initialization (Xavier/He)
- Forward pass: computes `Z = WX + b`, applies activation
- Backward pass: uses chain rule for gradients
- Supports `Sigmoid`, `ReLU`, and `Tanh`

### 2. `NeuralNetwork` â€“ `neural_network.py`
Manages architecture, training, and inference:
- Define custom architectures via layer sizes and activations
- Training loop with correct gradient flow
- Prediction and evaluation support

### 3. `Optimizer` â€“ `optimizers/mini_batch_sgd.py`
Implements Mini-batch SGD:
- Random shuffling and batching
- Parameter updates via gradient descent
- Configurable learning rate

### 4. `Loss` â€“ `losses/cross_entropy.py`
Binary cross-entropy loss function:
- Computes classification loss
- Provides gradient for output layer during backprop

---

## ðŸ§ª Usage

Run the example usage script:

```bash
python main.py
````


---

## ðŸ“¦ Installation

```bash
pip install -r requirements.txt
```

> Only NumPy is required.

---

## ðŸ“š Example

```python
from neural_network import NeuralNetwork

nn = NeuralNetwork(
    layer_sizes=[2, 4, 1],
    activations=['relu', 'sigmoid'],
    learning_rate=0.1,
    batch_size=2,
    epochs=1000
)

nn.train(X_train, y_train)
preds = nn.predict(X_test)
```


## ðŸ§  Why This Project?

To truly understand deep learning, building a neural network from the ground up is invaluable. This project gives you control over every detailâ€”activations, gradients, losses, and updatesâ€”so you can learn by doing.

---


